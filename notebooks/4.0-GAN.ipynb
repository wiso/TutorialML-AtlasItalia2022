{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informations\n",
    "The data is available in the CERN [OpenData](http://opendata.cern.ch/record/15012)\n",
    "The code is based on the FastCaloGAN code which is available on [Zenodo](https://zenodo.org/record/5589623) with the latest development available for ATLAS member on the [FCS git repository](https://gitlab.cern.ch/atlas-simulation-fastcalosim/fastcalogan)\n",
    "\n",
    "The data and code are also the case for the [#calochallenge](https://github.com/CaloChallenge/homepage)\n",
    "\n",
    "This example runs on only two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import DataLoader \n",
    "import importlib\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model\n",
    "from functools import partial\n",
    "tf.keras.backend.set_floatx('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = {\n",
    "    256: \"http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E256_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=bSSGDUvNFuCxHHz3YlCqga0Jq0g%3D&Expires=1829145580\",\n",
    "    512: \"http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E512_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=Dr3A32ycujBSm4bQ14l%2BHvEf1Ig%3D&Expires=1829145645\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GAN architecture\n",
    "The generator takes as input noise and in this case two values to create a conditional generator. It will output 368 values, using a stack of dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "noise = layers.Input(shape=(50), name=\"Noise\")\n",
    "condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "con = layers.concatenate([noise,condition])\n",
    "G = layers.Dense(50, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(con)  \n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(100, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(200, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "G = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(G)\n",
    "G = layers.BatchNormalization()(G)\n",
    "G = layers.Activation(activations.swish)(G)\n",
    "\n",
    "generator = Model(inputs=[noise, condition], outputs=G)\n",
    "generator.build(370)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Discriminator network\n",
    "The discriminator takes as input the values generated by the generator and check if they discriminate if they are real or generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.he_uniform()\n",
    "bias_node = True\n",
    "\n",
    "image = layers.Input(shape=(368), name=\"Image\")\n",
    "d_condition = layers.Input(shape=(2), name=\"mycond\")\n",
    "d_con = layers.concatenate([image, d_condition])\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(d_con)  \n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(368, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "D = layers.Activation(activations.relu)(D)\n",
    "D = layers.Dense(1, use_bias=bias_node, kernel_initializer=initializer, bias_initializer='zeros')(D)\n",
    "\n",
    "discriminator = Model(inputs=[image, d_condition], outputs=D)\n",
    "discriminator.build(370)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, loss and gradient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_penalty(f, x_real, x_fake, cond_label, batchsize, D):\n",
    "  alpha = tf.random.uniform([batchsize, 1], minval=0., maxval=1.)\n",
    "\n",
    "  inter = alpha * x_real + (1-alpha) * x_fake\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(inter)\n",
    "    pred = D(inputs=[inter, cond_label])\n",
    "  grad = t.gradient(pred, [inter])[0]\n",
    "  \n",
    "  slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "  gp = 0.00001 * tf.reduce_mean((slopes - 1.)**2) #Lambda\n",
    "  return gp\n",
    "\n",
    "@tf.function\n",
    "def D_loss(x_real, cond_label, batchsize, G, D): \n",
    "  z = tf.random.normal([batchsize, 50], mean=0.5, stddev=0.5, dtype=tf.dtypes.float32) #batch and latent dim\n",
    "  x_fake = G(inputs=[z, cond_label])\n",
    "  D_fake = D(inputs=[x_fake, cond_label])\n",
    "  D_real = D(inputs=[x_real, cond_label])\n",
    "  D_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real) + gradient_penalty(f=partial(D, training=True), x_real=x_real, x_fake=x_fake, cond_label=cond_label, batchsize=batchsize, D=D)\n",
    "  return D_loss, D_fake\n",
    "\n",
    "@tf.function\n",
    "def G_loss(D_fake):\n",
    "  G_loss = -tf.reduce_mean(D_fake)\n",
    "  return G_loss\n",
    "\n",
    "def getTrainData_ultimate( n_iteration, batchsize, dgratio, X ,Labels):\n",
    "  true_batchsize = tf.cast(tf.math.multiply(batchsize, dgratio), tf.int64)\n",
    "  n_samples = tf.cast(tf.gather(tf.shape(X), 0), tf.int64)\n",
    "  n_batch = tf.cast(tf.math.floordiv(n_samples, true_batchsize), tf.int64)\n",
    "  n_shuffles = tf.cast(tf.math.ceil(tf.divide(n_iteration, n_batch)), tf.int64)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((X, Labels))\n",
    "  ds = ds.shuffle(buffer_size = n_samples).repeat(n_shuffles).batch(true_batchsize, drop_remainder=True).prefetch(2)\n",
    "  return iter(ds)\n",
    "\n",
    "@tf.function\n",
    "def train_loop(X_trains, cond_labels, batchsize, dgratio, G, D, generator_optimizer, discriminator_optimizer): \n",
    "  for i in tf.range(dgratio):\n",
    "    print(\"d train: \" + str(i))\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "      (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, i), tf.gather(cond_labels, i), batchsize, G, D)\n",
    "      gradients_of_discriminator = disc_tape.gradient(D_loss_curr, D.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, D.trainable_variables))    \n",
    "      \n",
    "  print(\"g train\")\n",
    "  last_index = tf.subtract(dgratio, 1)\n",
    "\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    # Need to recompute D_fake, otherwise gen_tape doesn't know the history\n",
    "    (D_loss_curr, D_fake) = D_loss(tf.gather(X_trains, last_index), tf.gather(cond_labels, last_index), batchsize, G, D)\n",
    "    G_loss_curr = G_loss(D_fake)\n",
    "    gradients_of_generator = gen_tape.gradient(G_loss_curr, G.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, G.trainable_variables))\n",
    "    return D_loss_curr, G_loss_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgratio = 5\n",
    "batchsize = 128\n",
    "G_lr = D_lr = 0.0001\n",
    "G_beta1 = D_beta1 = 0.55\n",
    "generator_optimizer = tf.optimizers.Adam(learning_rate=G_lr, beta_1=G_beta1)\n",
    "discriminator_optimizer = tf.optimizers.Adam(learning_rate=D_lr, beta_1=D_beta1)\n",
    "\n",
    "# Prepare for check pointing\n",
    "saver = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                            discriminator_optimizer=discriminator_optimizer,\n",
    "                            generator=generator,\n",
    "                            discriminator=discriminator)\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader.DataLoader(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iteration = 0 \n",
    "max_iterations = 500\n",
    "\n",
    "for iteration in range(start_iteration,max_iterations): \n",
    "  change_data = (iteration == start_iteration)\n",
    "  \n",
    "  if (change_data == True):\n",
    "    X, Labels = dl.getAllTrainData(8, 9)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    Labels = tf.convert_to_tensor(Labels, dtype=tf.float32)\n",
    " \n",
    "    remained_iteration = tf.constant(max_iterations - iteration, dtype=tf.int64)\n",
    "    ds_iter = getTrainData_ultimate(remained_iteration, batchsize, dgratio, X ,Labels)\n",
    "    print (\"Using \"+ str(X.shape[0])+ \" events\")\n",
    "\n",
    "  X, Labels = ds_iter.get_next()\n",
    "\n",
    "  X_feature_size = tf.gather(tf.shape(X), 1)\n",
    "  Labels_feature_size = tf.gather(tf.shape(Labels), 1)\n",
    "  X_batch_shape = tf.stack((dgratio, batchsize, X_feature_size), axis=0)\n",
    "  Labels_batch_shape = tf.stack((dgratio, batchsize, Labels_feature_size), axis=0)\n",
    "\n",
    "  X_trains    = tf.reshape(X, X_batch_shape)\n",
    "  cond_labels = tf.reshape(Labels, Labels_batch_shape)  \n",
    "\n",
    "  #print(X_trains) \n",
    "  #print(cond_labels) \n",
    "  #print(batchsize) \n",
    "  #print(dgratio) \n",
    "  #generator.summary() \n",
    "  #discriminator.summary() \n",
    "\n",
    "  D_loss_curr, G_loss_curr = train_loop(X_trains, cond_labels, batchsize, dgratio, generator, discriminator,  generator_optimizer, discriminator_optimizer)\n",
    "\n",
    "  if iteration == 0: \n",
    "    print(\"Model and loss values will be saved every 2 iterations.\" )\n",
    "  \n",
    "  if iteration % 2 == 0 and iteration > 0:\n",
    "\n",
    "    try:\n",
    "      saver.save(file_prefix = checkpoint_dir+ '/model')\n",
    "    except:\n",
    "      print(\"Something went wrong in saving iteration %s, moving to next one\" % (iteration))\n",
    "      print(\"exception message \", sys.exc_info()[0])     \n",
    "    \n",
    "    print('Iter: {}; D loss: {:.4}; G_loss:  {:.4}'.format(iteration, D_loss_curr, G_loss_curr))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_best_checkpoints=\"best_iteration\"\n",
    "if not os.path.exists(output_best_checkpoints):\n",
    "  os.makedirs(output_best_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and load classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Noise (InputLayer)             [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 52)           0           ['Noise[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           2650        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 50)          200         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 50)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          5100        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 100)         400         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 100)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 200)          20200       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 200)         800         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 200)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 368)          73968       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 368)         1472        ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 368)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 104,790\n",
      "Trainable params: 103,354\n",
      "Non-trainable params: 1,436\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image (InputLayer)             [(None, 368)]        0           []                               \n",
      "                                                                                                  \n",
      " mycond (InputLayer)            [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 370)          0           ['Image[0][0]',                  \n",
      "                                                                  'mycond[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 368)          136528      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 368)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 368)          135792      ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 368)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 368)          135792      ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 368)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            369         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,481\n",
      "Trainable params: 408,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Loading data\n",
      "Opening file http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E256_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=bSSGDUvNFuCxHHz3YlCqga0Jq0g%3D&Expires=1829145580\n",
      "Loaded momentum 256 GeV with 10000 events and 368 columns\n",
      "Opening file http://rgw.fisica.unimi.it/TutorialML-AtlasItalia2022/gan_inputs/pid22_E512_eta_20_25_voxalisation.csv?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=Dr3A32ycujBSm4bQ14l%2BHvEf1Ig%3D&Expires=1829145645\n",
      "Loaded momentum 512 GeV with 10000 events and 368 columns\n",
      "Data was normalised by 256.000000\n",
      "Data was normalised by 512.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse \n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import ctypes\n",
    "import glob\n",
    "\n",
    "sys.path.append('gan_code/')\n",
    "import conditional_wgangp \n",
    "import importlib\n",
    "importlib.reload(conditional_wgangp)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "\n",
    "save_plots = True\n",
    "lparams = {'xoffset' : 0.1, 'yoffset' : 0.27, 'width'   : 0.8, 'height'  : 0.35}\n",
    "canvases = []   \n",
    "\n",
    "histos_vox = []\n",
    "input_files_vox = []\n",
    "\n",
    "particleName=\"#gamma\"\n",
    "particle=\"photons\"\n",
    "\n",
    "output_best_checkpoints=\"best_iteration\"\n",
    "if not os.path.exists(output_best_checkpoints):\n",
    "  os.makedirs(output_best_checkpoints)\n",
    "\n",
    "maxVoxel = 0\n",
    "midEnergy = 0\n",
    "step = 50\n",
    "\n",
    "wgan = conditional_wgangp.WGANGP()\n",
    "dl = DataLoader.DataLoader(fns)\n",
    "ekins = dl.ekins\n",
    "\n",
    "firstPosition = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build reference histos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{40, 450}\n",
      "40\n",
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "hist_lim = {\n",
    "    256: { 40, 450},\n",
    "    512: { 250, 750}\n",
    "}\n",
    "print(hist_lim[256])\n",
    "min,max=hist_lim[256]\n",
    "a,b={0, 10}\n",
    "print(min)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening vox files\n",
      "256\n",
      "40\n",
      "450\n",
      "512\n",
      "250\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "all_total_E = []\n",
    "\n",
    "print(\"Opening vox files\")\n",
    "for index, p in enumerate(ekins):  \n",
    "  df = pd.read_csv(fns[p], header=None, engine='python', dtype=np.float64)\n",
    "  #df = pd.read_csv(\"gan_inputs/pid22_E%d_eta_20_25_voxalisation.csv\"%(p), header=None, engine='python', dtype=np.float64)\n",
    "  df = df.fillna(0)\n",
    "  data=df.to_numpy()  \n",
    "\n",
    "  total_E = data.sum(axis=-1)\n",
    "  all_total_E.append(total_E)\n",
    "\n",
    "  min, max = hist_lim[p]\n",
    "  bins = np.linspace(min, max, 30)\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  plt.hist(total_E, bins=bins, label='reference', density=True,\n",
    "           histtype='stepfilled', alpha=0.2, linewidth=2.)\n",
    "  plt.legend(fontsize=20)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(\"plot_%d\"%(p), dpi=300)\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from 1 to 1000 in step of 50\n",
      "1\n",
      "checkpoints//model-1\n",
      "[8291.822 8312.508 8318.231 ... 8306.793 8315.206 8307.19 ]\n",
      "1\n",
      "checkpoints//model-1\n",
      "[16570.602 16573.752 16545.639 ... 16592.484 16622.441 16625.184]\n",
      "51\n",
      "checkpoints//model-51\n",
      "[4731.4326 4742.367  4694.384  ... 4719.254  4775.2524 4786.324 ]\n",
      "51\n",
      "checkpoints//model-51\n",
      "[9545.356 9507.4   9506.665 ... 9540.195 9510.289 9514.639]\n",
      "101\n",
      "checkpoints//model-101\n",
      "[3066.6711 3043.9248 3089.1206 ... 3094.606  3079.217  3060.179 ]\n",
      "101\n",
      "checkpoints//model-101\n",
      "[6168.661  6215.128  6204.4995 ... 6203.5786 6211.6255 6211.8984]\n",
      "151\n",
      "checkpoints//model-151\n",
      "[2561.2444 2520.8828 2522.9497 ... 2542.046  2532.902  2522.7617]\n",
      "151\n",
      "checkpoints//model-151\n",
      "[5144.5635 5156.461  5146.5146 ... 5124.867  5171.4805 5151.82  ]\n",
      "201\n",
      "checkpoints//model-201\n",
      "[2360.1152 2343.952  2388.2036 ... 2360.9512 2384.06   2366.825 ]\n",
      "201\n",
      "checkpoints//model-201\n",
      "[4862.584  4844.9976 4836.1475 ... 4832.8003 4820.1885 4846.0337]\n",
      "251\n",
      "checkpoints//model-251\n",
      "[2289.8772 2317.2617 2286.9712 ... 2272.1677 2284.0635 2306.996 ]\n",
      "251\n",
      "checkpoints//model-251\n",
      "[4671.543 4678.161 4637.776 ... 4665.8   4696.255 4661.093]\n",
      "301\n",
      "checkpoints//model-301\n",
      "[2270.0396 2234.1538 2244.0488 ... 2213.2283 2227.691  2228.2354]\n",
      "301\n",
      "checkpoints//model-301\n",
      "[4552.186  4577.2617 4548.0166 ... 4533.5537 4558.0703 4553.3535]\n",
      "351\n",
      "checkpoints//model-351\n",
      "[2225.1392 2226.1592 2216.3164 ... 2188.4758 2214.3286 2215.9238]\n",
      "351\n",
      "checkpoints//model-351\n",
      "[4494.65   4505.197  4500.7695 ... 4476.7666 4509.071  4523.8716]\n",
      "401\n",
      "checkpoints//model-401\n",
      "[2178.0684 2187.1787 2191.606  ... 2166.7173 2186.5352 2218.5579]\n",
      "401\n",
      "checkpoints//model-401\n",
      "[4430.87   4435.8726 4469.0835 ... 4464.064  4437.615  4437.536 ]\n",
      "451\n",
      "checkpoints//model-451\n",
      "[2179.6606 2159.8079 2150.3413 ... 2152.8298 2176.535  2193.497 ]\n",
      "451\n",
      "checkpoints//model-451\n",
      "[4434.901  4439.287  4456.516  ... 4428.8047 4419.116  4466.16  ]\n",
      "501\n",
      "checkpoints//model-501\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error when restoring from checkpoint or SavedModel at checkpoints//model-501: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints//model-501\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     91\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m   \u001b[1;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints//model-501",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   2536\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2537\u001b[1;33m       \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2538\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   2416\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcheckpoint_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheckpointOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2417\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2418\u001b[0m     metrics.AddCheckpointReadDuration(\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1423\u001b[1;33m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1424\u001b[0m     \u001b[0mgraph_building\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0merror_translator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     30\u001b[0m       'matching files for') in error_message:\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints//model-501",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-2e8570aad21e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnevents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'checkpoints/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;31m# * ekin_sample       #needed for conditional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\Documents\\Post-doc Roma\\ATLAS Italia\\TutorialML-AtlasItalia2022\\notebooks\\gan_code\\conditional_wgangp.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, epoch, labels, nevents, input_dir_gan)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0mcheckPointName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s/model-%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_dir_gan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckPointName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckPointName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnevents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   2539\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure restore operations have completed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2540\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2541\u001b[1;33m       raise errors_impl.NotFoundError(\n\u001b[0m\u001b[0;32m   2542\u001b[0m           \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2543\u001b[0m           \u001b[1;34mf\"Error when restoring from checkpoint or SavedModel at \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error when restoring from checkpoint or SavedModel at checkpoints//model-501: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints//model-501\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1')."
     ]
    }
   ],
   "source": [
    "import conditional_wgangp\n",
    "importlib.reload(conditional_wgangp)\n",
    "\n",
    "nevents=10000\n",
    "print (\"Running from %i to %i in step of %i\" %(1, 1000, step))\n",
    "for iteration in range(1, 1000, step):\n",
    "  legendPadIndex = 16\n",
    "  chi2_tot = 0.\n",
    "  ndf_tot = 0\n",
    "  input_files_gan = []\n",
    "\n",
    "  for index, energy in enumerate(ekins):     \n",
    "    ekin_sample = ekins[index]\n",
    "    energyArray = np.array([energy] * nevents)\n",
    "    etaArray = np.zeros(nevents) \n",
    "    labels = np.vstack((energyArray, etaArray)).T   \n",
    "\n",
    "    print(iteration)\n",
    "    data = wgan.load(iteration, labels, nevents, 'checkpoints/')\n",
    "    data = data * ekin_sample       #needed for conditional\n",
    "      \n",
    "    E_tot = data.numpy().sum(axis=1)\n",
    "    print(E_tot)\n",
    "\n",
    "    # Plotting\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.hist(all_total_E[index], bins=bins, label='reference', density=True,\n",
    "            histtype='stepfilled', alpha=0.2, linewidth=2.)\n",
    "    plt.hist(E_tot, bins=bins, label='reference', density=True,\n",
    "            histtype='stepfilled', alpha=0.2, linewidth=2.)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plot_%d_%d\"%(p,iteration), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN training and evaluation with 1M iterations\n",
    "The GAN learn but with significant fluctuations [plot](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SIMU-2018-04/fig_09.png)\n",
    "Animated gif with all energies [gif](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-SOFT-PUB-2020-006/fig_37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest results\n",
    "![pions](imgs/latest_pions.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba67b957603c6589b793cbc779fadd4d74491f4ed475d4948a7778f403f5ead2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
